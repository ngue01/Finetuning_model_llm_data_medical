# -*- coding: utf-8 -*-
"""fine-tunning_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPPJNL01m3J1EPPOgx9_LiSUVU85WHg6
"""

!pip install pyarrow==14.0.1
!pip install cudf-cu12
!pip install ibis-framework

! pip install -q -U jsonlines datasets transformers accelerate peft bitsandbytes wandb

import jsonlines
import itertools
import pandas as pd
import numpy as np
from pprint import pprint

import datasets
from datasets import load_dataset

import os
import torch
import time

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,  #concernant la tokinisation
    TrainingArguments,
    Trainer, #our notre entrainement de model
    GenerationConfig,
    pipeline #permet de mettre en place, plusiers ppipeline donc le tokenizer et les promts ainsi que le models
)

Device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(Device) #visualiser le divice utiliser

"""Data preparation
link:https://huggingface.co/datasets/prsdm/MedQuad-phi2-1k
"""

dataset = load_dataset("yahma/alpaca-cleaned", split="train", streaming=True) #fr

print(dataset) # notre jeux de donne dispose de 3 colonne

n = 10
print("Dataset:")
top_n = itertools.islice(dataset, n)
for i in top_n:
    print(i)

dat=pd.json_normalize(dataset) #convertir le fichier en dataframe

dat.head() #afficher les 5 premieres lignes, nous constatont que nous avons des valaurs manquante.

"""Avec ces trois colonnes, nous allons les conquatener afin d'avoir une structure bien definit

Use de promt template
"""

promt_template_whit_input = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
{instruction}
### Input:
{input}
### Response:
{output}

"""

#avec des valeur manquante, nous avons egalement introduit un second promt personnal sans la colonne input qui manque quels donnees
promt_template_without_input = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
{instruction}

### Response:
{output}
"""

def format_dataset(dataset,template_whit_input,template_without_input):
    formatted_data = []
    for entry in dataset:
        if entry["input"]:
          #application du template qui contient input
            formatted_entry = template_whit_input.format(instruction=entry["instruction"],input=entry["input"],output=entry["output"])
        else:
          #application du template qui ne contient pas la colonne input
            formatted_entry = template_without_input.format(instruction=entry["instruction"],output=entry["output"])
        formatted_data.append(formatted_entry)
    return formatted_data

#Formater ou refactoriser nos donnees du data
formatted_data = format_dataset(dataset,promt_template_whit_input,promt_template_without_input)

print(formatted_dataset[0])

for item in formatted_dataset:
    print(item)
    print()

"""#Realisons  du fine-tuning sur un Dataset Medical
liens:https://huggingface.co/datasets/prsdm/MedQuad-phi2-1k
"""

dataset = load_dataset("prsdm/MedQuad-phi2-1k", split="train")

print(dataset)

n = 10
print("dataset:")
top_n = itertools.islice(dataset, n)
for i in top_n:
    print(i)

"""## Telecharger le model a mettre sur place et tokenize nos donnees"""

from types import new_class
# model
base_model = "microsoft/phi-2" #appel du model de microsoft depuis hugging face
new_model = "phi-2-1a-kevin" #nom attribuer a mon futur model

#tokeniser
tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token # on donnes des zero au padding et
tokenizer.padding_side="right" # on donnes les zero au padding a droite


# Load model directly
# from transformers import AutoTokenizer, AutoModelForCausalLM

# tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
# model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2")

"""## Training

##Quantification configuration

Dans notre Dataset, nous pouvons avoir des mots qui sont representer sur 32 bits, 16 bits, grace a la quantification, nous pouvons chiffre notre model soit sur 4 bits ou sur 2 bits
"""

# HF_TOKEN = "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"

#quantization du model
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, #on charge
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)


# chargement du model
model = AutoModelForCausalLM.from_pretrained(
    base_model,  # Appel du modèle
    quantization_config=bnb_config,
    trust_remote_code=True,
    device_map={"": 0},  # Utilisation du GPU
)



model.config.use_cache = False
model.config.pretraining_tp = 1 #nombre de thread

print(model) #visualisation l'architecture model

"""## Configuration LoRA

*Configuration de LoRA

La configuration de LoRA (Low-Rank Adaptation) implique de spécifier des paramètres tels que le rang, l'alpha, le biais, le type de tâche et les modules cibles. Ces paramètres déterminent comment le modèle s'adapte pendant l'ajustement fin (fine-tuning).

* Rang (Rank) : Le rang des matrices de faible dimension utilisées dans l'adaptation.
* Alpha : Un facteur de mise à l'échelle pour les matrices de faible dimension.
* Biais (Bias) : Inclut ou exclut un terme de biais dans l'adaptation.
* Type de tâche (Task Type) : Le type spécifique de tâche pour laquelle l'adaptation est effectuée.
* Modules cibles (Target Modules) : Les modules du modèle que l'on souhaite adapter en utilisant LoRA.  


"""

from peft import LoraConfig, prepare_model_for_kbit_training

# Supposons que vous avez déjà un modèle chargé
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

from peft import LoraConfig, get_peft_model

# Configuration de LoRA
perf_config = LoraConfig(
    r=64, #nombre de rang
    lora_alpha=16, #
    lora_dropout=0.05, #
    bias="none", #
    task_type="CAUSAL_LM", # puisque on travaille avec un model baser juste sur les decodeur
    #different scification des couches sur les quels ont veux faire du fine-tuning
    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','dense','fc1','fc2',]
    )

# Application de la configuration de LoRA au modèle
model = get_peft_model(model, perf_config)
print_trainable_parameters(model)

"""## Etapes d'entrainement"""

# mon nouveau model qui sera phi-2-1a-kevin
new_model = "phi-2-1a-kevin"

# confi parametre
training_arguments = TrainingArguments(
    output_dir="./mon_model_resultat",
    num_train_epochs=1,
    per_device_train_batch_size=4, # Quantiter de donnees envoyer par entrainement
    gradient_accumulation_steps=1,

    gradient_checkpointing=True,
    max_grad_norm=0.3,

    learning_rate=2e-4,
    weight_decay=0.001,
    optim="paged_adamw_8bit",
    lr_scheduler_type="linear",
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    save_steps=0,
    logging_steps=15,
    run_name="phi-2-1a-kevin",
    overwrite_output_dir=True,
    report_to="wandb"
)

!pip install trl

from trl import SFTTrainer
#trainer

# Ensemble de parametre de supervision
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    eval_dataset=dataset,
    peft_config=perf_config,
    dataset_text_field="text",
    max_seq_length=512,
    tokenizer=tokenizer,
    args=training_arguments,
)

#Train model
trainer.train()

#sauvegarde du model
trainer.save_model(new_model)

